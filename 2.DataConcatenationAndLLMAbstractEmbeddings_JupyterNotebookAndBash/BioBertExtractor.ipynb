{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfa11a8-e562-4486-a2ff-ec6e27f217a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data and model prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e32b8-7350-4d1f-bc8b-162b6e38215b",
   "metadata": {},
   "source": [
    "### Check env is correctly configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acd23f0-1750-4481-814a-fb0c35ceed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/teghipco/.conda/envs/llm_v100/lib/python3.8/site-packages/torch/version.py'>\n",
      "12.4\n",
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.version); print(torch.version.cuda); print(torch.cuda.is_available()); print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290637bd-df53-45cc-ad54-9711804f13ae",
   "metadata": {},
   "source": [
    "### Download and save Biobert for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c967fb-5ba8-489f-b09c-34966b5eaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir ='/work/teghipco/LLMs/Biobert'\n",
    "import os\n",
    "os.chdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78c283b-8a6e-48fd-b877-0372664b86c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator() # multi-gpu\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = accelerator.prepare(model) # test accelerator/multi-gpu\n",
    "\n",
    "# Pipeline\n",
    "pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7e69b0-7522-480b-bae2-3f710e2cac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/work/teghipco/LLMs/Biobert/tokenizer_config.json',\n",
       " '/work/teghipco/LLMs/Biobert/special_tokens_map.json',\n",
       " '/work/teghipco/LLMs/Biobert/vocab.txt',\n",
       " '/work/teghipco/LLMs/Biobert/added_tokens.json',\n",
       " '/work/teghipco/LLMs/Biobert/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save locally\n",
    "pipe.model.save_pretrained(save_dir)\n",
    "pipe.tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279723f3-ca78-4669-8461-6bacd503eca3",
   "metadata": {},
   "source": [
    "### Concatenate sharded json files with publication-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e331992-63af-4d46-a1bc-ffb37261c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "json_directory = '/work/teghipco/LLMs/Biobert/data/pubs'\n",
    "json_files = glob.glob(os.path.join(json_directory, \"*.json\"))\n",
    "json_files = sorted(json_files, key=lambda x: int(re.search(r'(\\d+)', x).group())) # sort by shard\n",
    "#print(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90800c2-6ef5-41a3-87b0-bb069d5fd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "def load_json_file(file):\n",
    "    try:\n",
    "        print(f\"Processing file: {file}\") \n",
    "        dataframes = []\n",
    "        with gzip.open(file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                df = pd.json_normalize(data)\n",
    "                dataframes.append(df)\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "478feb80-01a5-4a00-b8dd-1faa9423a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE--TESTING\n",
    "#del(result_dataframes)\n",
    "#del(test_full_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3260f2ed-0324-4f59-a8a3-3f9323375d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000002.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000000.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000003.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000001.json\n",
      "\n",
      "\n",
      "\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000004.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000005.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000006.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000007.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000008.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000009.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000010.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000011.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000012.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000013.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000014.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000015.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000016.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000017.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000018.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000019.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000020.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000021.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000022.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000023.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000024.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000025.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000026.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000027.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000028.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000029.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000030.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000031.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000032.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000033.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000034.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000035.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000036.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000037.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000038.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000039.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000040.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000041.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000042.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000043.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000044.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000045.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000046.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000047.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000048.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000049.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000050.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000053.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000051.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000052.json\n",
      "\n",
      "\n",
      "\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000054.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000055.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000056.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000057.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000058.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000059.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000060.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000061.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000062.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000063.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000064.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000065.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000066.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000067.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000068.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000069.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000070.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000071.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000072.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000073.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000074.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000075.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000076.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000077.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000078.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000079.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000080.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000081.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000082.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000083.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000084.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000085.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000086.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000087.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000088.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000089.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000090.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000091.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000092.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000093.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000094.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000095.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000096.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000097.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000098.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000099.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000101.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000102.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000100.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000103.json\n",
      "\n",
      "\n",
      "\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000104.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000105.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000106.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000107.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000108.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000109.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000110.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000111.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000112.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000113.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000114.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000115.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000116.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000117.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000118.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000119.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000120.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000121.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000122.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000123.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000124.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000125.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000126.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000127.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000128.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000129.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000130.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000131.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000132.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000133.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000134.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000135.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000136.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000137.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000138.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000139.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000140.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000141.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000142.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000143.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000144.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000145.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000146.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000147.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000148.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000149.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000150.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000153.jsonProcessing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000151.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000152.json\n",
      "\n",
      "\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000154.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000155.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000156.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000157.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000158.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000159.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000160.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000161.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000162.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000163.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000164.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000165.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000166.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000167.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000168.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000169.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000170.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000171.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000172.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000173.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000174.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000175.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000176.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000177.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000178.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000179.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000180.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000181.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000182.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000183.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000184.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000185.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000186.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000187.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000188.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000189.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000190.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000191.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000192.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000193.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000194.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000195.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000196.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000197.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000198.json\n",
      "Processing file: /work/teghipco/LLMs/Biobert/data/pubs/pubWhole_export_000000000199.json\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50  # Process files in batches--faster on my particular HPC setup ymmv\n",
    "result_dataframes = []\n",
    "\n",
    "for i in range(0, len(json_files), batch_size):\n",
    "    batch_files = json_files[i:i + batch_size]\n",
    "    #with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    with ProcessPoolExecutor as executor:\n",
    "        batch_dataframes = list(executor.map(load_json_file, batch_files))\n",
    "        result_dataframes.append(pd.concat(batch_dataframes, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c9b63e-470d-47e1-9ba5-504197c31ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publication_id  year        date date_online  \\\n",
      "0  pub.1142700518  2021  2021-11-19  2021-11-19   \n",
      "1  pub.1157734354  2023  2023-05-04  2023-05-04   \n",
      "2  pub.1176144398  2024  2024-09-30  2024-09-30   \n",
      "3  pub.1133314038  2020  2020-11-01         NaN   \n",
      "4  pub.1168937635  2024  2024-02-06         NaN   \n",
      "\n",
      "                                     title_preferred clinical_trial_ids  \\\n",
      "0  P11‐17: Outcomes of COVID 19 critical care pat...                      \n",
      "1  The Bronte Creek Project: Outdoor Environmenta...                      \n",
      "2  (Mis)recognising the symbolic violence of acad...                      \n",
      "3                                  Lunar dust buster                      \n",
      "4               Lessons from COVID-19: UK experience                      \n",
      "\n",
      "  times_cited recent_citations publication_type  is_open_access  ...  \\\n",
      "0           0                0          article            True  ...   \n",
      "1           0                0          chapter            True  ...   \n",
      "2           0                0          article            True  ...   \n",
      "3           0                0          article            True  ...   \n",
      "4           0                0          chapter            True  ...   \n",
      "\n",
      "                                  abstract_preferred  \\\n",
      "0                                                NaN   \n",
      "1  The Bronte Creek Project (BCP) was a unique en...   \n",
      "2  This paper contributes to critical scholarship...   \n",
      "3  Any astronauts thinking of living on the Moon ...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                book_title_preferred altmetric_score  \\\n",
      "0                                                NaN             NaN   \n",
      "1  Outdoor Environmental Education in the Contemp...             NaN   \n",
      "2                                                NaN             8.0   \n",
      "3                                                NaN             NaN   \n",
      "4                           Post Keynesian Economics             NaN   \n",
      "\n",
      "  field_citation_ratio field proceedings_title_preferred conference_name  \\\n",
      "0                  NaN   NaN                         NaN             NaN   \n",
      "1                  NaN   NaN                         NaN             NaN   \n",
      "2                  NaN   NaN                         NaN             NaN   \n",
      "3                  0.0   NaN                         NaN             NaN   \n",
      "4                  NaN   NaN                         NaN             NaN   \n",
      "\n",
      "  citation_count title_original  relative_citation_ratio  \n",
      "0            NaN            NaN                      NaN  \n",
      "1            NaN            NaN                      NaN  \n",
      "2            NaN            NaN                      NaN  \n",
      "3            NaN            NaN                      NaN  \n",
      "4            NaN            NaN                      NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate into one dataframe\n",
    "full_dataframe = pd.concat(result_dataframes, ignore_index=True)\n",
    "print(full_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90865ba4-c285-4e9e-a39d-89f5a61052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = '/work/teghipco/LLMs/Biobert/pub_data_concat.parquet'\n",
    "full_dataframe.to_parquet(save_df, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3517259-e406-4c92-a886-9fd43e321336",
   "metadata": {},
   "source": [
    "# Load prepared data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcbd0b-3af2-4d22-8ab4-5f17af1ff324",
   "metadata": {},
   "source": [
    "#### Assuming you are coming back to the notebook (new kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d6503-d812-49a4-8036-3ca777611c2c",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccb241c-b48c-4757-ae9f-f00ac04eafaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publication_id  year        date date_online  \\\n",
      "0  pub.1142700518  2021  2021-11-19  2021-11-19   \n",
      "1  pub.1157734354  2023  2023-05-04  2023-05-04   \n",
      "2  pub.1176144398  2024  2024-09-30  2024-09-30   \n",
      "3  pub.1133314038  2020  2020-11-01        None   \n",
      "4  pub.1168937635  2024  2024-02-06        None   \n",
      "\n",
      "                                     title_preferred clinical_trial_ids  \\\n",
      "0  P11‐17: Outcomes of COVID 19 critical care pat...                      \n",
      "1  The Bronte Creek Project: Outdoor Environmenta...                      \n",
      "2  (Mis)recognising the symbolic violence of acad...                      \n",
      "3                                  Lunar dust buster                      \n",
      "4               Lessons from COVID-19: UK experience                      \n",
      "\n",
      "  times_cited recent_citations publication_type  is_open_access  ...  \\\n",
      "0           0                0          article            True  ...   \n",
      "1           0                0          chapter            True  ...   \n",
      "2           0                0          article            True  ...   \n",
      "3           0                0          article            True  ...   \n",
      "4           0                0          chapter            True  ...   \n",
      "\n",
      "                                  abstract_preferred  \\\n",
      "0                                               None   \n",
      "1  The Bronte Creek Project (BCP) was a unique en...   \n",
      "2  This paper contributes to critical scholarship...   \n",
      "3  Any astronauts thinking of living on the Moon ...   \n",
      "4                                               None   \n",
      "\n",
      "                                book_title_preferred altmetric_score  \\\n",
      "0                                               None             NaN   \n",
      "1  Outdoor Environmental Education in the Contemp...             NaN   \n",
      "2                                               None             8.0   \n",
      "3                                               None             NaN   \n",
      "4                           Post Keynesian Economics             NaN   \n",
      "\n",
      "  field_citation_ratio field proceedings_title_preferred conference_name  \\\n",
      "0                  NaN  None                        None            None   \n",
      "1                  NaN  None                        None            None   \n",
      "2                  NaN  None                        None            None   \n",
      "3                  0.0  None                        None            None   \n",
      "4                  NaN  None                        None            None   \n",
      "\n",
      "  citation_count title_original  relative_citation_ratio  \n",
      "0            NaN           None                      NaN  \n",
      "1            NaN           None                      NaN  \n",
      "2            NaN           None                      NaN  \n",
      "3            NaN           None                      NaN  \n",
      "4            NaN           None                      NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "save_df = '/work/teghipco/LLMs/Biobert/pub_data_concat.parquet'\n",
    "full_dataframe = pd.read_parquet(save_df)\n",
    "print(full_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea2f3a-a309-4430-801d-c77e4412488f",
   "metadata": {},
   "source": [
    "#### Extract just the abstracts to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6206f2-be5d-4650-aee0-5e674a48f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = full_dataframe['abstract_preferred'].dropna().tolist()\n",
    "del full_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f1eca-4493-4231-9453-4bf3bf1042c6",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369a7fde-07a3-4d04-a62f-8dc9ad7d8c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/teghipco/.conda/envs/llm_v100/lib/python3.8/site-packages/torch/version.py'>\n",
      "12.4\n",
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.version); print(torch.version.cuda); print(torch.cuda.is_available()); print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e39d88d-6734-457b-9c0f-8a37684acec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case this was previously run...\n",
    "try:\n",
    "    del(abstract_embeddings)\n",
    "except NameError:\n",
    "    pass # Does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e0350a-2a36-4618-9f8c-083d5f7e16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also in case you were messing around with models previously...(this is not the first attempt there is much data to optimize for!:) )\n",
    "import gc\n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()  # Clear any remaining memory cache\n",
    "    gc.collect()  # Run garbage collection to free up RAM as well\n",
    "except NameError:\n",
    "    pass # Not loaded yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ffb8b0-3c49-4330-b7dd-a33ec91c55cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "save_dir = '/work/teghipco/LLMs/Biobert'\n",
    "model = AutoModel.from_pretrained(save_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "\n",
    "# Enable data parallelism through cuda instead of accelerate (see scratch; faster)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa00be3-b11f-48a2-9438-815c4f86137e",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6518da9d-ed3e-4101-872a-ce83a7735fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "346cd2f0-89d9-4dd8-b7b9-230325f7a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and other parameters\n",
    "batch_size = 32\n",
    "max_length = 512\n",
    "expected_embedding_dim = 768\n",
    "checkpoint_path = \"/work/teghipco/LLMs/Biobert/checkpoint.pkl\"  # Path to save progress\n",
    "abstract_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd42c7-a470-4d2f-93e7-004f3b1c1ae0",
   "metadata": {},
   "source": [
    "#### Load data from an existing checkpoint if you ran the main loop previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71713878-3934-47ff-8164-a6822430cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from batch 2240000.\n"
     ]
    }
   ],
   "source": [
    "start_batch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "        abstract_embeddings = checkpoint[\"embeddings\"]\n",
    "        start_batch = checkpoint[\"abstract_index\"]\n",
    "    print(f\"Resuming from batch {start_batch}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95490195-109c-47f1-8213-060a5c60c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2240000 embeddings, resuming from batch 2240000.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(abstract_embeddings)} embeddings, resuming from batch {start_batch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b788f98-ad5c-49ab-934c-38d1e6108e30",
   "metadata": {},
   "source": [
    "#### Helper function to get CLS embedding from model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71942994-e3f6-48ef-b3b1-2e7a455613d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(batch_texts):\n",
    "    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    inputs = {key: val.to('cuda') for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4287e-3090-4d92-a27a-47b9fcd481ff",
   "metadata": {},
   "source": [
    "#### Loop over batches of abstracts and get model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b4f715-bd8b-4a42-9f8c-81058ab6018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/8055 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First abstract after resuming from batch 2240000:\n",
      "The role of healthcare and climate change in sustainable development is crucial for ensuring a healthy and prosperous future for people and the planet. Addressing the health impacts of climate change requires a multi-faceted approach that includes reducing greenhouse gas emissions to mitigate the drivers of climate change, enhancing adaptation strategies to build resilience in communities, and implementing public health measures to protect vulnerable populations. Collaboration between governments, international organizations, healthcare providers, and communities is essential in confronting the health challenges posed by climate change. Healthcare and climate change are interconnected and play pivotal roles in sustainable development. A holistic approach that considers the health impacts of climate change promotes sustainable healthcare practices, and addresses health disparities is vital for achieving a sustainable and resilient future for all. In this chapter, we are going to discuss some strategies for climate change and health policies that can reduce the burden of illness related to climate change and work towards a healthier and more sustainable future for all.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 8055/8055 [36:09<00:00,  3.71it/s] \n",
      "/tmp/ipykernel_177541/2721809186.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789116784/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  abstract_embeddings = torch.tensor(abstract_embeddings)\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(start_batch, len(abstracts), batch_size), desc=\"Processing Batches\"):\n",
    "    \n",
    "    if i == start_batch:\n",
    "        print(f\"First abstract after resuming from batch {start_batch}:\")\n",
    "        print(abstracts[i])  # Print the first abstract after resuming (debug)\n",
    "    \n",
    "    batch_texts = []\n",
    "\n",
    "    # Chunk long abstracts or process in one go\n",
    "    for abstract in abstracts[i:i + batch_size]:\n",
    "        tokens = tokenizer(abstract, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "        input_ids = tokens['input_ids'][0]\n",
    "\n",
    "        if len(input_ids) > max_length:\n",
    "            # Split into 512-token chunks and add to batch_texts\n",
    "            for j in range(0, len(input_ids), max_length):\n",
    "                chunk_input_ids = input_ids[j:j+max_length]\n",
    "                # Decode each chunk to pass into the pipeline as text\n",
    "                chunk_text = tokenizer.decode(chunk_input_ids, skip_special_tokens=True)\n",
    "                batch_texts.append(chunk_text)\n",
    "        else:\n",
    "            # Short abstracts can be added directly\n",
    "            batch_texts.append(abstract)\n",
    "\n",
    "    # Extract embeddings for the batch\n",
    "    batch_embeddings = get_cls_embedding(batch_texts)\n",
    "\n",
    "    # Aggregate and store embeddings\n",
    "    idx = 0\n",
    "    for abstract in abstracts[i:i + batch_size]:\n",
    "        tokens = tokenizer(abstract, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "        input_ids = tokens['input_ids'][0]\n",
    "        \n",
    "        if len(input_ids) > max_length:\n",
    "            # Number of chunks in this abstract\n",
    "            num_chunks = len(input_ids) // max_length + (len(input_ids) % max_length > 0)\n",
    "            cls_embedding = np.mean(batch_embeddings[idx:idx + num_chunks], axis=0)\n",
    "            idx += num_chunks\n",
    "        else:\n",
    "            cls_embedding = batch_embeddings[idx]\n",
    "            idx += 1\n",
    "\n",
    "        abstract_embeddings.append(cls_embedding)\n",
    "\n",
    "    # Save checkpoint every 10000 batches to safeguard progress\n",
    "    if (i // batch_size + 1) % 10000 == 0:\n",
    "        with open(checkpoint_path, \"wb\") as f:\n",
    "            pickle.dump({\"embeddings\": abstract_embeddings, \"abstract_index\": i + batch_size}, f)\n",
    "        print(f\"Checkpoint saved at batch {i + batch_size}.\")\n",
    "\n",
    "# Convert embeddings to tensor after final batch\n",
    "abstract_embeddings = torch.tensor(abstract_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ed2d37c-50c4-479d-8b8f-8fa63f53f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embed = np.array(abstract_embeddings)\n",
    "save_path = \"/work/teghipco/LLMs/Biobert/abstract_embeddings.npy\"\n",
    "np.save(save_path, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad79994-c46a-482f-827d-371115fd5a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2497751"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed2 = np.load(save_path, allow_pickle=True)\n",
    "len(embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5cc906-a26f-45e0-b64d-bda225adb228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2497751"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae8715-835b-4062-b12d-89d1f87a37fd",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0facf38-9374-4ad6-976a-ebc1432c1773",
   "metadata": {},
   "source": [
    "### Slower pipeline alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51003167-9c12-435f-8d47-778bf54205ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "#from accelerate import Accelerator\n",
    "#save_dir ='/work/teghipco/LLMs/Biobert'  # Replace with your actual save path\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "#model = AutoModel.from_pretrained(save_dir)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "#model = accelerator.prepare(model)\n",
    "#pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbefe9-c5b3-4e56-acdd-bd3273c50e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This is a test input to verify the pipeline is functioning correctly.\"\n",
    "features = pipe(test_text)\n",
    "print(\"Number of tokens:\", len(features[0]))\n",
    "print(\"Embedding vector size for each token:\", len(features[0][0]))\n",
    "print(\"First token embedding:\", features[0][0])  # Embedding for [CLS] token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
